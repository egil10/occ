{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ML Model Comparison: Detecting Elite Players\n",
                "\n",
                "This notebook demonstrates the usage of our 10 foundational ML model scripts.\n",
                "We will attempt to predict if a player is an \"Elite\" player (Top 10% by Wins) based on their PvP and Objective metrics.",
                "\n",
                "**Goal:** Compare 5 Classic vs 5 Advanced Models.\n",
                "**Setup:** Fixed Seed (808), 60/20/20 Split."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import sys\n",
                "import os\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "# --- Path Setup ---\n",
                "# Ensure we can import from 'ml' folder regardless of where notebook is run\n",
                "current_dir = os.getcwd()\n",
                "project_root = current_dir\n",
                "\n",
                "# If we are in 'notebooks' folder, go up one level\n",
                "if os.path.basename(current_dir) == 'notebooks':\n",
                "    project_root = os.path.dirname(current_dir)\n",
                "\n",
                "if project_root not in sys.path:\n",
                "    sys.path.append(project_root)\n",
                "\n",
                "# --- Imports ---\n",
                "# Now we import using the package structure\n",
                "from ml.utils import get_splitter, evaluate_classifier, SEED\n",
                "from ml.visuals import set_style, save_plot, get_palette\n",
                "\n",
                "# Import Models\n",
                "from ml.logistic_regression import get_model as get_logreg\n",
                "from ml.knn import get_model as get_knn\n",
                "from ml.decision_tree import get_model as get_dt\n",
                "from ml.random_forest import get_model as get_rf\n",
                "from ml.svm import get_model as get_svm\n",
                "\n",
                "from ml.xgboost_model import get_model as get_xgb\n",
                "from ml.lightgbm_model import get_model as get_lgbm\n",
                "from ml.gradient_boosting import get_model as get_gbm\n",
                "from ml.neural_network import get_model as get_nn\n",
                "from ml.adaboost import get_model as get_ada\n",
                "\n",
                "# Apply Theme\n",
                "set_style()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load Golden Dataset - Robust path handling\n",
                "data_path = os.path.join(project_root, 'data', 'processed', 'golden_dataset.parquet')\n",
                "df = pd.read_parquet(data_path)\n",
                "\n",
                "# Create Target Variable: \"Elite\" (Top 10% of Wins)\n",
                "# We use 'wins_matches' as the success metric\n",
                "target_metric = 'wins_matches'\n",
                "threshold = df[target_metric].quantile(0.90)\n",
                "df['is_elite'] = (df[target_metric] >= threshold).astype(int)\n",
                "\n",
                "print(f\"Dataset Shape: {df.shape}\")\n",
                "print(f\"Elite Players: {df['is_elite'].sum()} ({df['is_elite'].mean():.1%})\")\n",
                "print(f\"Features Available: {list(df.columns)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Preprocessing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Drop Non-Numeric / ID columns and the Target source column to avoid leakage\n",
                "# We want to predict \"Match Winning Ability\" using ONLY PvP and Objective stats.\n",
                "# So we drop all match-related columns (wins, losses, matches, etc.) as they are directly leaks.\n",
                "drop_cols = [\n",
                "    'player_id', \n",
                "    'wins_matches', 'losses_matches', 'matches_matches', 'wl_ratio_matches', 'ties_matches', # Target Leakage\n",
                "    'position_matches', 'position_pvp', 'position_objectives' # Rank Leakage\n",
                "]\n",
                "\n",
                "# Robust column dropping (only drop what exists)\n",
                "existing_drop_cols = [c for c in drop_cols if c in df.columns]\n",
                "X_df = df.drop(columns=existing_drop_cols)\n",
                "\n",
                "# Filter for numeric\n",
                "X_df = X_df.select_dtypes(include=[np.number])\n",
                "\n",
                "# Fill NaNs (important for LogReg/NeuralNet)\n",
                "X_df = X_df.fillna(0)\n",
                "\n",
                "# Setup Target\n",
                "y = df['is_elite']\n",
                "\n",
                "# Combine for splitter\n",
                "full_data = X_df.copy()\n",
                "full_data['target'] = y\n",
                "\n",
                "print(f\"Training with {X_df.shape[1]} Features: {list(X_df.columns)}\")\n",
                "\n",
                "X_train, X_val, X_test, y_train, y_val, y_test = get_splitter(full_data, 'target')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Training Loop"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "models = {\n",
                "    \"Logistic Regression\": get_logreg(),\n",
                "    \"KNN\": get_knn(),\n",
                "    \"Decision Tree\": get_dt(),\n",
                "    \"Random Forest\": get_rf(),\n",
                "    \"SVM\": get_svm(),\n",
                "    \"XGBoost\": get_xgb(),\n",
                "    \"LightGBM (HistGB)\": get_lgbm(),\n",
                "    \"Gradient Boosting\": get_gbm(),\n",
                "    \"Neural Network\": get_nn(),\n",
                "    \"AdaBoost\": get_ada()\n",
                "}\n",
                "\n",
                "results = []\n",
                "\n",
                "for name, model in models.items():\n",
                "    if model is None:\n",
                "        print(f\"Skipping {name} (Not available)\")\n",
                "        continue\n",
                "        \n",
                "    print(f\"Training {name}...\")\n",
                "    try:\n",
                "        model.fit(X_train, y_train)\n",
                "        \n",
                "        # Predict on Validation set for comparison\n",
                "        preds = model.predict(X_val)\n",
                "        \n",
                "        # Get probabilities if supported (for ROC AUC)\n",
                "        probs = None\n",
                "        if hasattr(model, \"predict_proba\"):\n",
                "            probs = model.predict_proba(X_val)\n",
                "        \n",
                "        metrics = evaluate_classifier(y_val, preds, probs, model_name=name)\n",
                "        results.append(metrics)\n",
                "        \n",
                "    except Exception as e:\n",
                "        print(f\"Error training {name}: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Results Comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "results_df = pd.DataFrame(results).set_index(\"Model\")\n",
                "results_df = results_df.sort_values(\"F1 Score\", ascending=False)\n",
                "\n",
                "# Display Table\n",
                "print(results_df)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualization\n",
                "plt.figure(figsize=(12, 6))\n",
                "sns.barplot(x=results_df.index, y=results_df['F1 Score'], palette=get_palette())\n",
                "plt.title(\"Model Comparison - F1 Score (Elite Player Detection)\")\n",
                "plt.xticks(rotation=45, ha='right')\n",
                "plt.ylim(0, 1.0)\n",
                "plt.xlabel(\"\")\n",
                "plt.ylabel(\"F1 Score\")\n",
                "\n",
                "# Save Plot\n",
                "save_path = os.path.join(project_root, 'plots', 'model_evaluation')\n",
                "save_plot('model_f1_comparison', folder=save_path)\n",
                "\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Test Best Model\n",
                "Selecting the best model based on Validation F1 Score and running on the held-out Test Set."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "best_model_name = results_df.index[0]\n",
                "print(f\"Best Model: {best_model_name}\")\n",
                "\n",
                "best_model = models[best_model_name]\n",
                "# Predict on Test\n",
                "test_preds = best_model.predict(X_test)\n",
                "test_metrics = evaluate_classifier(y_test, test_preds, model_name=best_model_name + \" (Test)\")\n",
                "\n",
                "pd.DataFrame([test_metrics])"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}